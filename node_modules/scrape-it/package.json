{
  "_from": "scrape-it",
  "_id": "scrape-it@5.0.5",
  "_inBundle": false,
  "_integrity": "sha512-H+qXFE2xN3NQtSc1p6sQCt0EFahrQwhVtlcvxkXCf5y0Pn7AeE5XCrHo6mTuQmWN+cge6LlD8rV+UcoAQRvq7A==",
  "_location": "/scrape-it",
  "_phantomChildren": {},
  "_requested": {
    "type": "tag",
    "registry": true,
    "raw": "scrape-it",
    "name": "scrape-it",
    "escapedName": "scrape-it",
    "rawSpec": "",
    "saveSpec": null,
    "fetchSpec": "latest"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/scrape-it/-/scrape-it-5.0.5.tgz",
  "_shasum": "0ae9282243df012163d6f4c367d34ef100473996",
  "_spec": "scrape-it",
  "_where": "/Users/xavier/GitHub/bootcamp/promo1808/bonus/scrapping-leboncoin",
  "author": {
    "name": "Ionică Bizău",
    "email": "bizauionica@gmail.com",
    "url": "https://ionicabizau.net"
  },
  "blah": {
    "h_img": "https://i.imgur.com/j3Z0rbN.png",
    "cli": "scrape-it-cli",
    "installation": [
      {
        "h2": "FAQ"
      },
      {
        "p": "Here are some frequent questions and their answers."
      },
      {
        "h3": "1. How to parse scrape pages?"
      },
      {
        "p": "`scrape-it` has only a simple request module for making requests. That means you cannot directly parse ajax pages with it, but in general you will have those scenarios:"
      },
      {
        "ol": [
          "**The ajax response is in JSON format.** In this case, you can make the request directly, without needing a scraping library.",
          "**The ajax response gives you HTML back.** Instead of calling the main website (e.g. example.com), pass to `scrape-it` the ajax url (e.g. `example.com/api/that-endpoint`) and you will you will be able to parse the response",
          "**The ajax request is so complicated that you don't want to reverse-engineer it.** In this case, use a headless browser (e.g. Google Chrome, Electron, PhantomJS) to load the content and then use the `.scrapeHTML` method from scrape it once you get the HTML loaded on the page."
        ]
      },
      {
        "h3": "2. Crawling"
      },
      {
        "p": "There is no fancy way to crawl pages with `scrape-it`. For simple scenarios, you can parse the list of urls from the initial page and then, using Promises, parse each page. Also, you can use a different crawler to download the website and then use the `.scrapeHTML` method to scrape the local files."
      },
      {
        "h3": "3. Local files"
      },
      {
        "p": "Use the `.scrapeHTML` to parse the HTML read from the local files using `fs.readFile`."
      }
    ]
  },
  "bugs": {
    "url": "https://github.com/IonicaBizau/scrape-it/issues"
  },
  "bundleDependencies": false,
  "contributors": [
    {
      "name": "ComFreek",
      "email": "comfreek@outlook.com",
      "url": "https://github.com/ComFreek"
    }
  ],
  "dependencies": {
    "assured": "^1.0.10",
    "cheerio": "^0.20.0",
    "cheerio-req": "^1.2.0",
    "err": "^2.1.8",
    "is-empty-obj": "^1.0.9",
    "iterate-object": "^1.3.2",
    "obj-def": "^1.0.6",
    "typpy": "^2.3.9"
  },
  "deprecated": false,
  "description": "A Node.js scraper for humans.",
  "devDependencies": {
    "@types/cheerio": "^0.22.6",
    "lien": "^1.0.1",
    "tester": "^1.4.1"
  },
  "files": [
    "bin/",
    "app/",
    "lib/",
    "dist/",
    "src/",
    "scripts/",
    "resources/",
    "menu/",
    "cli.js",
    "index.js",
    "bloggify.js",
    "bloggify.json",
    "bloggify/"
  ],
  "homepage": "https://github.com/IonicaBizau/scrape-it#readme",
  "keywords": [
    "scrape",
    "it",
    "a",
    "scraping",
    "module",
    "for",
    "humans"
  ],
  "license": "MIT",
  "main": "lib/index.js",
  "name": "scrape-it",
  "repository": {
    "type": "git",
    "url": "git+ssh://git@github.com/IonicaBizau/scrape-it.git"
  },
  "scripts": {
    "test": "node test"
  },
  "types": "lib/index.d.ts",
  "version": "5.0.5"
}
